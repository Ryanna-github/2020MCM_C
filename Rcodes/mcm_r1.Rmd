---
title: "hair_dryer"
author: "renyan"
date: "2020/3/6"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 读取数据


```{r}
setwd("D:/Ryanna/mcm_real/Rcodes")
hd <- read.csv('../Data/hair_dryer.csv', stringsAsFactors = F)
head(hd)
```

# 取出所有评论文本构成一个文档

```{r}
reviews <- hd$review_body
class(reviews)
length(reviews)
write.table(reviews, '../Data/hair_dryer_reviews.txt', col.names = FALSE, quote = FALSE, row.names = FALSE)
```


# LDA 部分

<!-- ```{r} -->
<!-- # 文件分词 -->
<!-- cutter <- worker(bylines = T) -->
<!-- comments_seg <- cutter["../Data/hair_dryer_reviews.txt"] -->
<!-- ``` -->

<!-- ```{r} -->
<!-- comments_segged<- readLines("./reviews.txt",encoding="UTF-8") #读取分词结果 -->
<!-- comments <- as.list(comments_segged) #将向量转化为列表 -->
<!-- doc.list <- strsplit(as.character(comments),split=" ") #将每行文本，按照空格分开，每行变成一个词向量，储存在列表里 -->
<!-- ``` -->


<!-- ```{r} -->
<!-- term.table <- table(unlist(doc.list)) -->
<!-- #这里有两步，unlist用于统计每个词的词频；table把结果变成一个交叉表式的factor，原理类似python里的词典，key是词，value是词频 -->
<!-- term.table <- sort(term.table, decreasing = TRUE) #按照词频降序排列 -->
<!-- # 这里储存为table类型，表示在所有评论中的词频 -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # 删去出现频率小于5以及单词长度小于2的词 -->
<!-- del <- term.table < 5| nchar(names(term.table))<2   #把不符合要求的筛出来 -->
<!-- term.table <- term.table[!del]   #去掉不符合要求的 -->
<!-- vocab <- names(term.table)    #创建词库 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- get.terms <- function(x) { -->
<!-- index <- match(x, vocab)  # 获取词的ID -->
<!-- index <- index[!is.na(index)]  #去掉没有查到的，也就是去掉了的词 -->
<!-- rbind(as.integer(index - 1), as.integer(rep(1, length(index))))   #生成对应结构 -->
<!-- } -->
<!-- documents <- lapply(doc.list, get.terms) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- K <- 3   #主题数 -->
<!-- G <- 5000    #迭代次数 -->
<!-- alpha <- 0.10    -->
<!-- eta <- 0.02 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(lda) -->
<!-- set.seed(357) -->
<!-- fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, num.iterations = G, alpha = alpha, eta = eta, initial = NULL, burnin = 0, compute.log.likelihood = TRUE) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))  #文档—主题分布矩阵 -->
<!-- phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))  #主题-词语分布矩阵 -->
<!-- term.frequency <- as.integer(term.table)   #词频 -->
<!-- doc.length <- sapply(documents, function(x) sum(x[2, ])) #每篇文章的长度，即有多少个词 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(LDAvis) -->
<!-- json <- createJSON(phi = phi, theta = theta, -->
<!--                doc.length = doc.length, vocab = vocab, -->
<!--                term.frequency = term.frequency) -->
<!-- #json为作图需要数据，下面用servis生产html文件，通过out.dir设置保存位置 -->
<!-- serVis(json, out.dir = '../vis2', open.browser = FALSE) -->
<!-- ``` -->


# 情感分析


- 用python得到情感得分直接读入结果进行拼接并输出

```{r}
setwd("D:/Ryanna/mcm_real/Rcodes")
hd_st <- read.csv('../Data/hair_dryer_senti.csv', stringsAsFactors = F)[,2]
# 拼接
names(hd_st)
hd$pos_score <- hd_st
# write.csv(hd, '../Data/hair_dryer_withsenti.csv')
```


- 对含有great的评论进行特殊处理

```{r}
great_idx = grepl("great", tolower(hd$review_body))
hd$pos_score[great_idx] = mean(hd$pos_score) + rnorm(1, 0, 0.3)
# ggplot(hd, aes(x = pos_score)) + geom_histogram()
```


- 进一步数据筛选，情感得分失败的样本舍弃，并且生成新的关于有效性的贝叶斯指标

```{r}
# library(data.table)
# library(plyr)
hd <- hd[!is.na(hd$pos_score),]
averageratio<-sum(hd$helpful_votes)/sum(hd$total_votes)
hd$bys_weight<-(100*averageratio+hd$helpful_votes)/(100+hd$total_votes)
hd$bys_pos_score <- hd$bys_weight * hd$pos_score
```


# 综合指标的得分

- 熵值法给定权重


```{r}
# 归一化处理
min.max.norm <- function(x){
  (x-min(x))/(max(x)-min(x))
}

# 求出所有样本对指标Xj的贡献总量
first1 <- function(data)
{
  x <- c(data)
  for(i in 1:length(data))
    x[i] = data[i]/sum(data[])
  return(x)
}

# 将上步生成的矩阵每个元素变成每个元素与该ln（元素）的积并计算信息熵。
first2 <- function(data)
{
  x <- c(data)
  for(i in 1:length(data)){
    if(data[i] == 0){
      x[i] = 0
    }else{
      x[i] = data[i] * log(data[i])
    }
  }
  return(x)
}

mv_weight <- apply(hd[,c("bys_pos_score", "star_rating")],2,min.max.norm)
dataframe <- apply(mv_weight,2,first1)
dataframe1 <- apply(dataframe,2,first2)
k <- 1/log(length(dataframe1[,1]))
d <- -k * colSums(dataframe1)

# 计算冗余度
d <- 1-d
# 计算各项指标权重
w <- d/sum(d)
w
```

- 计算综合指标得分

```{r}
hd$score <- w['bys_pos_score']*hd$bys_pos_score + w['star_rating']*hd$star_rating
```


# 时间类型转化

```{r}
hd$review_date <- as.Date(hd$review_date, "%m/%d/%Y")
```


至此，数据处理部分结束

```{r}
setwd("D:/Ryanna/mcm_real/Rcodes")
write.csv(hd, '../Data/hair_dryer_withsenti.csv', row.names = F)
```


# 按照分数划分评论

- 先看看分数的大致分布

```{r}
summary(hd$score)
```


- 好评差评划分

```{r}
# 按照评分划分
# midscore <- 2
# hd_good_reviews <- hd[hd$score >= midscore, 'review_body']
# hd_bad_reviews <- hd[hd$score < midscore, 'review_body']
```

```{r}
# 按照时间划分
good_start <- as.Date('2015/2/1')
good_end <- as.Date('2015/4/30')
bad_start <- as.Date('2006/1/1')
bad_end <- as.Date('2006/11/30')

hd_good_reviews <- hd[hd$review_date >= good_start & hd$review_date <= good_end, 'review_body']
hd_bad_reviews <- hd[hd$review_date >= bad_start & hd$review_date <= bad_end, 'review_body']
```

- 储存好评差评数据数据

```{r}
setwd("D:/Ryanna/mcm_real/Rcodes")
write.table(hd_good_reviews, '../Data/hair_dryer_reviews_good.txt', col.names = FALSE, quote = FALSE, row.names = FALSE)
write.table(hd_bad_reviews, '../Data/hair_dryer_reviews_bad.txt', col.names = FALSE, quote = FALSE, row.names = FALSE)
```



# 使用tf-idf寻找特征

- 在 python 中进行转换

```{r}
top_n <- 100
```

```{r}
setwd("D:/Ryanna/mcm_real/Rcodes")
hd_good_ti <- read.csv('../Data/hair_dryer_tfidf_good.csv', stringsAsFactors = F)
hd_bad_ti <- read.csv('../Data/hair_dryer_tfidf_bad.csv', stringsAsFactors = F)
```


```{r}
word <- names(apply(hd_good_ti[,-1], 2, sum))
g <- data.frame('word' = word, 'sum' = apply(hd_good_ti[,-1], 2, sum))
b <- data.frame('word' = word, 'sum' = apply(hd_bad_ti[,-1], 2, sum))
zuocha <- data.frame('word' = word, 'goodsum_badsum' = g$sum - b$sum)
gf_word <- head(zuocha[order(zuocha$goodsum_badsum, decreasing = T),], top_n)$word
bf_word <- head(zuocha[order(zuocha$goodsum_badsum, decreasing = F),], top_n)$word
```



```{r}
setwd("D:/Ryanna/mcm_real/Rcodes")
hd_good_ti <- read.csv('../Data/hair_dryer2_tfidf_good.csv', stringsAsFactors = F)
hd_bad_ti <- read.csv('../Data/hair_dryer2_tfidf_bad.csv', stringsAsFactors = F)
```

```{r}
word <- names(apply(hd_good_ti[,-1], 2, sum))
g <- data.frame('word' = word, 'sum' = apply(hd_good_ti[,-1], 2, sum))
b <- data.frame('word' = word, 'sum' = apply(hd_bad_ti[,-1], 2, sum))
zuocha <- data.frame('word' = word, 'goodsum_badsum' = g$sum - b$sum)
gf_word2 <- head(zuocha[order(zuocha$goodsum_badsum, decreasing = T),], top_n)$word
bf_word2 <- head(zuocha[order(zuocha$goodsum_badsum, decreasing = F),], top_n)$word
```


```{r}
gb_word <- data.frame(time_based_good = gf_word,
                      time_based_bad = bf_word,
                      class_based_good = gf_word2,
                      class_based_bad = bf_word2)
gb_word
```


# 标明各词性单词数目

```{r}
# 先写出此时评论
# 与之前条数不同，需要重新写出
setwd("D:/Ryanna/mcm_real/Rcodes")
write.table(hd$review_body, '../Data/hair_dryer_reviews_less.txt', col.names = FALSE, quote = FALSE, row.names = FALSE)
```


```{r}
setwd("D:/Ryanna/mcm_real/Rcodes")
hd_tags <- read.csv('../Data/hair_dryer_tags.csv', stringsAsFactors = F)
hd$adj_num <- hd_tags$adj_num
hd$vb_num <- hd_tags$vb_num
hd$n_num <- hd_tags$n_num
```






```{r}
hd$star_rating <- as.numeric(hd$star_rating)
sum(hd$star_rating)/dim(hd)[1]
pc$star_rating <- as.numeric(pc$star_rating)
sum(pc$star_rating)/dim(pc)[1]
mv$star_rating <- as.numeric(mv$star_rating)
sum(mv$star_rating)/dim(mv)[1]
```







